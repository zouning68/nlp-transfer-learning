import os
from absl import flags

pwd_path = os.path.abspath(os.path.dirname(__file__))

class Config:
    def __init__(self):
        self.jd = os.path.join(pwd_path, "data/jd")
        self.jd_data = os.path.join(pwd_path, "data/example/jd.txt")

conf = Config()

FLAGS = flags.FLAGS
#************************************ preprocess data_utils.py parameters ******************************************#
flags.DEFINE_bool("use_tpu", False, help="whether to use TPUs")
flags.DEFINE_integer("bsz_per_host", 32, help="batch size per host.")
flags.DEFINE_integer("num_core_per_host", 1, help="num TPU cores per host.")
flags.DEFINE_integer("seq_len", 512, help="Sequence length.")
flags.DEFINE_integer("reuse_len", 256, help="Number of token that can be reused as memory. Could be half of `seq_len`.")
flags.DEFINE_bool("uncased", True, help="Use uncased inputs or not.")
flags.DEFINE_bool("bi_data", True, help="whether to create bidirectional data")
flags.DEFINE_integer("mask_alpha", default=6, help="How many tokens to form a group.")
flags.DEFINE_integer("mask_beta", default=1, help="How many tokens to mask within each group.")
flags.DEFINE_bool("use_eod", True, help="whether to append EOD at the end of a doc.")
flags.DEFINE_bool("from_raw_text", True, help="Whether the input is raw text or encoded ids.")
flags.DEFINE_integer("num_predict", default=85, help="Num of tokens to predict.")
flags.DEFINE_string("input_glob", "data/example/*.txt", help="Input file glob.")
flags.DEFINE_string("sp_path", "models/spiece.model", help="Path to the sentence piece model.")
flags.DEFINE_string("save_dir", "proc_data/example", help="Directory for saving the processed data.")
flags.DEFINE_enum("split", "train", ["train", "dev", "test"], help="Save the data as which split.")
flags.DEFINE_integer("pass_id", 0, help="ID of the current pass. Different passes sample different negative segment.")
flags.DEFINE_integer("num_task", 1, help="Number of total tasks.")
flags.DEFINE_integer("task", 0, help="The Task ID. This value is used when using multiple workers to identify each worker.")
#************************************ pretrain train.py parameters ******************************************#
flags.DEFINE_integer("perm_size", 256, help="Window size of permutation.")
flags.DEFINE_integer("n_token", 32000, help="Vocab size")
flags.DEFINE_string("model_dir", default="pretrain_model", help="Estimator model_dir.")
flags.DEFINE_integer("train_batch_size", default=32, help="Size of the train batch across all hosts.")
flags.DEFINE_string("record_info_dir", default="proc_data/example/tfrecords", help="Path to local directory containing `record_info-lm.json`.")
flags.DEFINE_integer("num_hosts", default=1, help="number of TPU hosts")
flags.DEFINE_integer("num_passes", default=1, help="Number of passed used for training.")
flags.DEFINE_bool("use_bfloat16", False, help="Whether to use bfloat16.")
flags.DEFINE_integer("mem_len", default=384, help="Number of steps to cache")
flags.DEFINE_string("master", default=None, help="master")
flags.DEFINE_integer("iterations", default=1000, help="Number of iterations per repeat loop.")
flags.DEFINE_integer("max_save", default=10, help="Maximum number of checkpoints to save.")
flags.DEFINE_integer("save_steps", default=5, help="Number of steps for model checkpointing. None for not saving checkpoints")
flags.DEFINE_integer("train_steps", default=1000, help="Total number of training steps.")
flags.DEFINE_integer("batch_size", default=2, help="batch size of every train step")
flags.DEFINE_integer("n_layer", default=2, help="Number of layers.")
flags.DEFINE_integer("d_model", default=100, help="Dimension of the model.")
flags.DEFINE_integer("n_head", default=4, help="Number of attention heads.")
flags.DEFINE_integer("d_head", default=8, help="Dimension of each attention head.")
flags.DEFINE_integer("d_inner", default=128, help="Dimension of inner hidden size in positionwise feed-forward.")
flags.DEFINE_string("ff_activation", default="relu", help="Activation type used in position-wise feed-forward.")
flags.DEFINE_bool("untie_r", default=True, help="Untie r_w_bias and r_r_bias")
flags.DEFINE_float("dropout", default=0.5, help="Dropout rate.")
flags.DEFINE_float("dropatt", default=0.1, help="Attention dropout rate.")
flags.DEFINE_enum("init", default="normal", enum_values=["normal", "uniform"], help="Initialization method.")
flags.DEFINE_float("init_range", default=0.1, help="Initialization std when init is uniform.")
flags.DEFINE_float("init_std", default=0.02, help="Initialization std when init is normal.")
flags.DEFINE_integer("clamp_len", default=-1, help="Clamp length")
flags.DEFINE_bool("same_length", default=False, help="Same length attention")
flags.DEFINE_integer("warmup_steps", default=10000, help="Number of steps for linear lr warmup.")
flags.DEFINE_float("learning_rate", default=1e-4, help="Maximum learning rate.")
flags.DEFINE_string("decay_method", default="poly", help="Poly or cos.")
flags.DEFINE_float("min_lr_ratio", default=0.001, help="Minimum ratio learning rate.")
flags.DEFINE_float("weight_decay", default=0.0, help="Weight decay rate.")
flags.DEFINE_float("adam_epsilon", default=1e-8, help="Adam epsilon.")
flags.DEFINE_float("clip", default=1.0, help="Gradient clipping value.")
flags.DEFINE_string("init_checkpoint", default="pretrain_model", help="Checkpoint path for initializing the model.")